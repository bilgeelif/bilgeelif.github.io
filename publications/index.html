<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>readings | Elif Bilge</title> <meta name="author" content="Elif Bilge"> <meta name="description" content="These publications are the ones that I enjoy most and learn from."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%B8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://bilgeelif.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Elif </span>Bilge</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">teaching</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">readings<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">readings</h1> <p class="post-description">These publications are the ones that I enjoy most and learn from.</p> </header> <article> <div class="publications"> <h2 class="year">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://arxiv.org/abs/2304.02643" rel="external nofollow noopener" target="_blank">Segment Any.</a></abbr></div> <div id="arXiv:2004.08790" class="col-sm-8"> <div class="title">Segment Anything</div> <div class="author"> Alexander Kirillov, Eric Mintun, Nikhila Ravi, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em></em> Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/segAny.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>"Segment Anything" is a technical paper that presents a novel approach for segmenting arbitrary objects in images and videos. The proposed method combines deep learning techniques with traditional computer vision algorithms to achieve state-of-the-art results in a variety of segmentation tasks. The authors introduce a new architecture called "Segmentation Transformer Network" (STN), which is designed to handle segmentation tasks in a more efficient and effective manner. The paper also includes a detailed analysis of the STN’s performance on several benchmarks, demonstrating its ability to outperform existing methods in terms of accuracy and speed. Overall, "Segment Anything" represents a significant advance in the field of computer vision, and has the potential to enable a wide range of applications in areas such as robotics, autonomous driving, and augmented reality.</p> </div> </div> </div> </li></ol> <h2 class="year">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://arxiv.org/abs/2010.11929" rel="external nofollow noopener" target="_blank">VIT</a></abbr></div> <div id="arXiv:2004.08791" class="col-sm-8"> <div class="title">AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</div> <div class="author"> Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em></em> Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/VIT.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" is a research paper that introduces a new approach for image recognition using transformers, a type of deep learning model that has achieved remarkable success in natural language processing tasks. The paper presents a novel architecture, called Vision Transformer (ViT), which applies the transformer model to images by treating them as sequences of patches. ViT achieves state-of-the-art results on multiple image recognition benchmarks, including ImageNet, with fewer parameters than previous approaches. The technical aspects of the paper include the details of the ViT architecture, the pre-training and fine-tuning procedures, and the ablation studies that investigate the impact of different components on the model’s performance. The paper also discusses the limitations and future directions of the approach, highlighting the potential of transformers for advancing the field of computer vision.</p> </div> </div> </div> </li></ol> <h2 class="year">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://arxiv.org/abs/2004.08790" rel="external nofollow noopener" target="_blank">UNet3</a></abbr></div> <div id="arXiv:2004.08792" class="col-sm-8"> <div class="title">UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation</div> <div class="author"> Huimin Huang, Lanfen Lin, Ruofeng Tong, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Hongjie Hu, Qiaowei Zhang, Yutaro Iwamoto, Xianhua Han, Yen-Wwi Chen, Jian Wu' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em></em> Apr 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/unet3+.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Deep learning-based approaches have revolutionized medical image segmentation, with the U-Net architecture being one of the most widely used for this task. In their recent paper, Cheng et al. propose a novel architecture called UNet3+, which integrates full-scale skip connections and a hybrid attention mechanism into the U-Net architecture to achieve more accurate segmentation results. The full-scale skip connections enable information to be propagated between different levels of the network, allowing both low-level and high-level features to be captured more effectively. The hybrid attention mechanism is a combination of channel attention and spatial attention modules, which allows the network to focus on the most informative regions in the input image. The authors evaluated UNet3+ on several public medical imaging datasets, including the BraTS and LiTS datasets, and achieved state-of-the-art performance in terms of segmentation accuracy, sensitivity, specificity, and Dice coefficient. The proposed UNet3+ architecture provides a powerful tool for medical image segmentation and has the potential to improve clinical diagnosis and treatment planning.</p> </div> </div> </div> </li></ol> <h2 class="year">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">Attention</a></abbr></div> <div id="arXiv:2004.08793" class="col-sm-8"> <div class="title">Attention Is All You Need</div> <div class="author"> Ashish Vaswani, Noam Shazeer, Niki Parmar, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em></em> Dec 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/attention.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The "Attention is All You Need" paper proposes a novel sequence-to-sequence model architecture, called the Transformer, that relies solely on self-attention mechanisms for encoding and decoding sequences. Unlike traditional sequence models, the Transformer is able to parallelize training and inference, making it more efficient for long sequences. The paper also introduces a new training objective, called "label smoothing," which regularizes the model’s predictions and improves generalization. The authors demonstrate the effectiveness of the Transformer on a variety of natural language processing tasks, achieving state-of-the-art results on machine translation and language modeling benchmarks. Overall, the paper presents a significant advance in sequence modeling and has since become a foundational work in the field of deep learning.</p> </div> </div> </div> </li></ol> <h2 class="year">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://arxiv.org/abs/1506.01497" rel="external nofollow noopener" target="_blank">Faster R-CNN</a></abbr></div> <div id="arXiv:2004.08794" class="col-sm-8"> <div class="title">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</div> <div class="author"> Shaoqing Ren, Kaiming He, Ross Girshick, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jian Sun' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> Jan 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/fasterRcnn.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks" is a technical paper that introduces a new object detection framework that is faster and more accurate than existing methods. The proposed method builds on the popular R-CNN (Region-based Convolutional Neural Network) approach, but incorporates a Region Proposal Network (RPN) to generate object proposals more efficiently. The RPN shares convolutional layers with the detection network, allowing for end-to-end training and significantly reducing computation time. The paper includes a detailed description of the RPN architecture, as well as experimental results demonstrating its superiority over previous object detection methods on several benchmark datasets. The proposed method achieves state-of-the-art results while running at near real-time speeds, making it well-suited for applications such as autonomous driving, robotics, and video analysis. Overall, "Faster R-CNN" represents a significant advance in the field of computer vision, and has the potential to enable a wide range of practical applications.</p> </div> </div> </div> </li></ol> <h2 class="year">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://arxiv.org/abs/1502.03167" rel="external nofollow noopener" target="_blank">Batch Norm.</a></abbr></div> <div id="arXiv:2004.08795" class="col-sm-8"> <div class="title">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</div> <div class="author"> Sergey Ioffe, and Christian Szegedy</div> <div class="periodical"> <em></em> Dec 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/batch.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The paper addresses the problem of internal covariate shift, which occurs when the distribution of inputs to a layer in a deep neural network changes as the network parameters are updated during training, leading to slow convergence and poor performance. The paper proposes a technique called batch normalization, which normalizes the inputs to a layer by subtracting the batch mean and dividing by the batch standard deviation, thereby reducing the internal covariate shift. The paper also describes how to incorporate batch normalization into a deep network by adding batch normalization layers after each fully connected or convolutional layer. The technical aspects of the paper include an in-depth analysis of the effects of internal covariate shift on deep network training, a detailed description of the batch normalization technique and its implementation, and experimental results demonstrating the effectiveness of batch normalization in improving the convergence rate and generalization performance of deep neural networks. The paper has had a significant impact on the field of deep learning, and batch normalization has become a standard technique used in many state-of-the-art deep network architectures.</p> </div> </div> </div> </li></ol> <h2 class="year">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://arxiv.org/abs/1412.6980" rel="external nofollow noopener" target="_blank">ADAM</a></abbr></div> <div id="arXiv:2004.08796" class="col-sm-8"> <div class="title">ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION</div> <div class="author"> Diederik P. Kingma, and Jimmy Ba</div> <div class="periodical"> <em></em> Dec 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/adam.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The paper addresses the problem of slow convergence and poor generalization that can occur with traditional optimization algorithms, such as stochastic gradient descent.The technical aspects of the paper include a detailed description of the ADAM algorithm, which combines the advantages of two other popular optimization algorithms, momentum and RMSprop. ADAM calculates an adaptive learning rate for each parameter based on the first and second moments of the gradient, allowing it to converge quickly and efficiently to a solution.The paper also includes an analysis of the properties of the ADAM algorithm, including its convergence properties and robustness to hyperparameters. Experimental results demonstrate that ADAM achieves state-of-the-art performance on a range of benchmark datasets and deep network architectures.Overall, the paper has had a significant impact on the field of deep learning, and ADAM has become one of the most widely used optimization algorithms for training deep neural networks. Its technical contributions include the introduction of a novel optimization algorithm, a detailed analysis of its properties, and experimental results demonstrating its effectiveness.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Elif Bilge. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: December 13, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>