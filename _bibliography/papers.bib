---
---

@string{aps = {American Physical Society,}}

@article{arXiv:2004.08790,
  abbr={Segment Any.}, 
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi,Nikhila and Mao,Hanzi and Rolland,Chloe and Gustafson,Laura and Xiao,Tete and Whitehead,Spencer and C. Berg,Alexander and Lo,Wan-Yen  and Doll√°r, Piotr and Girshick, Ross },
  abstract={"Segment Anything" is a technical paper that presents a novel approach for segmenting arbitrary objects in images and videos. The proposed method combines deep learning techniques with traditional computer vision algorithms to achieve state-of-the-art results in a variety of segmentation tasks. The authors introduce a new architecture called "Segmentation Transformer Network" (STN), which is designed to handle segmentation tasks in a more efficient and effective manner. The paper also includes a detailed analysis of the STN's performance on several benchmarks, demonstrating its ability to outperform existing methods in terms of accuracy and speed. Overall, "Segment Anything" represents a significant advance in the field of computer vision, and has the potential to enable a wide range of applications in areas such as robotics, autonomous driving, and augmented reality.}, 
  year={2023},
  month={April},
  publisher=aps,
  url={https://arxiv.org/abs/2304.02643},
  pdf={segAny.pdf},
  selected={true}
}

@article{arXiv:2004.08790,
  abbr={VIT}, 
  title={AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE},
  author={Dosovitskiy, Alexey and Beyer,Lucas and Kolesnikov,Alexander and Weissenborn,Dirk and Zhai,Xiaohua and Unterthiner,Thomas and Dehghani,Mostafa and Minderer,Matthias and Heigold,Georg and Gelly,Sylvain and Uszkoreit,Jakob and Houlsby,Neil},   
  abstract={"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" is a research paper that introduces a new approach for image recognition using transformers, a type of deep learning model that has achieved remarkable success in natural language processing tasks. The paper presents a novel architecture, called Vision Transformer (ViT), which applies the transformer model to images by treating them as sequences of patches. ViT achieves state-of-the-art results on multiple image recognition benchmarks, including ImageNet, with fewer parameters than previous approaches. The technical aspects of the paper include the details of the ViT architecture, the pre-training and fine-tuning procedures, and the ablation studies that investigate the impact of different components on the model's performance. The paper also discusses the limitations and future directions of the approach, highlighting the potential of transformers for advancing the field of computer vision.}, 
  year={2021},
  month={June},
  publisher=aps,
  url={https://arxiv.org/abs/2010.11929},
  pdf={VIT.pdf},
  selected={false}
}

@article{arXiv:2004.08790,
  abbr={UNet3}, 
  title={UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation},
  author={Huang,Huimin and Lin,Lanfen and Tong,Ruofeng and Hu,Hongjie and Zhang,Qiaowei and Iwamoto,Yutaro and Han,Xianhua and Chen,Yen-Wwi and Wu,Jian},  
  abstract={Deep learning-based approaches have revolutionized medical image segmentation, with the U-Net architecture being one of the most widely used for this task. In their recent paper, Cheng et al. propose a novel architecture called UNet3+, which integrates full-scale skip connections and a hybrid attention mechanism into the U-Net architecture to achieve more accurate segmentation results. The full-scale skip connections enable information to be propagated between different levels of the network, allowing both low-level and high-level features to be captured more effectively. The hybrid attention mechanism is a combination of channel attention and spatial attention modules, which allows the network to focus on the most informative regions in the input image. The authors evaluated UNet3+ on several public medical imaging datasets, including the BraTS and LiTS datasets, and achieved state-of-the-art performance in terms of segmentation accuracy, sensitivity, specificity, and Dice coefficient. The proposed UNet3+ architecture provides a powerful tool for medical image segmentation and has the potential to improve clinical diagnosis and treatment planning.},
  year={2020},
  month={April},
  publisher=aps,
  url={https://arxiv.org/abs/2004.08790},
  pdf={unet3+.pdf},
  dimensions={true},
  selected={true}
}

@article{arXiv:2004.08790,
  abbr={Attention}, 
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer,Noam and Parmar,Niki and Uszkoreit,Jakob and Jones,Llion and Gomez,Aidan N. and Kaiser,Lukasz and Polosukhin,Illia},  
  abstract={The "Attention is All You Need" paper proposes a novel sequence-to-sequence model architecture, called the Transformer, that relies solely on self-attention mechanisms for encoding and decoding sequences. Unlike traditional sequence models, the Transformer is able to parallelize training and inference, making it more efficient for long sequences. The paper also introduces a new training objective, called "label smoothing," which regularizes the model's predictions and improves generalization. The authors demonstrate the effectiveness of the Transformer on a variety of natural language processing tasks, achieving state-of-the-art results on machine translation and language modeling benchmarks. Overall, the paper presents a significant advance in sequence modeling and has since become a foundational work in the field of deep learning.},
  year={2017},
  month={December},
  publisher=aps,
  url={https://arxiv.org/abs/1706.03762},
  pdf={attention.pdf},
  selected={false}
}

@article{arXiv:2004.08790,
  abbr={Faster R-CNN}, 
  title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick,Ross and Sun,Jian},
  abstract={"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks" is a technical paper that introduces a new object detection framework that is faster and more accurate than existing methods. The proposed method builds on the popular R-CNN (Region-based Convolutional Neural Network) approach, but incorporates a Region Proposal Network (RPN) to generate object proposals more efficiently. The RPN shares convolutional layers with the detection network, allowing for end-to-end training and significantly reducing computation time. The paper includes a detailed description of the RPN architecture, as well as experimental results demonstrating its superiority over previous object detection methods on several benchmark datasets. The proposed method achieves state-of-the-art results while running at near real-time speeds, making it well-suited for applications such as autonomous driving, robotics, and video analysis. Overall, "Faster R-CNN" represents a significant advance in the field of computer vision, and has the potential to enable a wide range of practical applications.},
  year={2016},
  month={January},
  publisher=aps,
  url={https://arxiv.org/abs/1506.01497},
  pdf={fasterRcnn.pdf},
  selected={false}
}

@article{arXiv:2004.08790,
  abbr={Batch Norm.}, 
  title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author={Ioffe,Sergey and  Szegedy,Christian},
  abstract={The paper addresses the problem of internal covariate shift, which occurs when the distribution of inputs to a layer in a deep neural network changes as the network parameters are updated during training, leading to slow convergence and poor performance.

The paper proposes a technique called batch normalization, which normalizes the inputs to a layer by subtracting the batch mean and dividing by the batch standard deviation, thereby reducing the internal covariate shift. The paper also describes how to incorporate batch normalization into a deep network by adding batch normalization layers after each fully connected or convolutional layer.

The technical aspects of the paper include an in-depth analysis of the effects of internal covariate shift on deep network training, a detailed description of the batch normalization technique and its implementation, and experimental results demonstrating the effectiveness of batch normalization in improving the convergence rate and generalization performance of deep neural networks. The paper has had a significant impact on the field of deep learning, and batch normalization has become a standard technique used in many state-of-the-art deep network architectures.}, 
  year={2015},
  month={December},
  publisher=aps,
  url={https://arxiv.org/abs/1412.6980},
  pdf={batch.pdf},
  selected={false}
}

@article{arXiv:2004.08790,
  abbr={ADAM}, 
  title={ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION},
  author={Kingma,Diederik P. and Ba, Jimmy},
  abstract={The paper addresses the problem of slow convergence and poor generalization that can occur with traditional optimization algorithms, such as stochastic gradient descent.The technical aspects of the paper include a detailed description of the ADAM algorithm, which combines the advantages of two other popular optimization algorithms, momentum and RMSprop. ADAM calculates an adaptive learning rate for each parameter based on the first and second moments of the gradient, allowing it to converge quickly and efficiently to a solution.The paper also includes an analysis of the properties of the ADAM algorithm, including its convergence properties and robustness to hyperparameters. Experimental results demonstrate that ADAM achieves state-of-the-art performance on a range of benchmark datasets and deep network architectures.Overall, the paper has had a significant impact on the field of deep learning, and ADAM has become one of the most widely used optimization algorithms for training deep neural networks. Its technical contributions include the introduction of a novel optimization algorithm, a detailed analysis of its properties, and experimental results demonstrating its effectiveness.}, 
  year={2014},
  month={December},
  publisher=aps,
  url={https://arxiv.org/abs/1412.6980},
  pdf={adam.pdf},
  selected={false}
}

